{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWVJr0owXhD0saY0ACSlSC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranao0609/DL_Projects/blob/main/12_Machine_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-iU7acyNx-XR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    (\"hello\", \"salut\"),\n",
        "    (\"how are you\", \"comment Ã§a va\"),\n",
        "    (\"i am fine\", \"je vais bien\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"goodbye\", \"au revoir\")\n",
        "]"
      ],
      "metadata": {
        "id": "3ieIGOyXyGMn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    tokens = set(\" \".join(sentences).split())\n",
        "    word2idx = {word: i+2 for i, word in enumerate(tokens)}\n",
        "    word2idx[\"<pad>\"] = 0\n",
        "    word2idx[\"<eos>\"] = 1\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "eng_sentences, fr_sentences = zip(*pairs)\n",
        "eng2idx, idx2eng = build_vocab(eng_sentences)\n",
        "fr2idx, idx2fr = build_vocab(fr_sentences)"
      ],
      "metadata": {
        "id": "XhXqXq9TyGPd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(sentence, vocab):\n",
        "    return [vocab[word] for word in sentence.split()] + [vocab[\"<eos>\"]]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.data = [(encode(e, eng2idx), encode(f, fr2idx)) for e, f in pairs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    src = nn.utils.rnn.pad_sequence(src, batch_first=True)\n",
        "    tgt = nn.utils.rnn.pad_sequence(tgt, batch_first=True)\n",
        "    return src, tgt\n",
        "\n",
        "loader = DataLoader(TranslationDataset(pairs), batch_size=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "nRew2eSWyGSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "29jbA1UJyGVm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        return self.fc(out), hidden"
      ],
      "metadata": {
        "id": "vBzGp1FFyWgC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(eng2idx)\n",
        "OUTPUT_DIM = len(fr2idx)\n",
        "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "n5g-svwJyZGI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    for src, tgt in loader:\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        hidden = encoder(src)\n",
        "        dec_input = tgt[:, :-1]\n",
        "        target = tgt[:, 1:]\n",
        "\n",
        "        outputs, _ = decoder(dec_input, hidden)\n",
        "        loss = criterion(outputs.reshape(-1, OUTPUT_DIM), target.reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OClry9Ayb_n",
        "outputId": "20acd1c3-cd94-4dd5-bf93-f992db09be26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3457\n",
            "Epoch 2, Loss: 1.1959\n",
            "Epoch 3, Loss: 0.4806\n",
            "Epoch 4, Loss: 0.1328\n",
            "Epoch 5, Loss: 0.0432\n",
            "Epoch 6, Loss: 0.0172\n",
            "Epoch 7, Loss: 0.0085\n",
            "Epoch 8, Loss: 0.0050\n",
            "Epoch 9, Loss: 0.0033\n",
            "Epoch 10, Loss: 0.0025\n",
            "Epoch 11, Loss: 0.0019\n",
            "Epoch 12, Loss: 0.0016\n",
            "Epoch 13, Loss: 0.0014\n",
            "Epoch 14, Loss: 0.0012\n",
            "Epoch 15, Loss: 0.0011\n",
            "Epoch 16, Loss: 0.0010\n",
            "Epoch 17, Loss: 0.0010\n",
            "Epoch 18, Loss: 0.0009\n",
            "Epoch 19, Loss: 0.0009\n",
            "Epoch 20, Loss: 0.0008\n"
          ]
        }
      ]
    }
  ]
}